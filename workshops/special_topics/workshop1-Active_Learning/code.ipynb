{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W1_Active_Learning_2_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tidaltamu/workshops/blob/main/special_topics/workshop1/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD0E3z0DB08t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28bZ1nqCY0m"
      },
      "source": [
        "What is active learning?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBRzPR4CaA_"
      },
      "source": [
        "Active learning is a ML technique in which we use a portion of labelled data and interactively and continously label new data points to improve the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enK1ukZICoQh"
      },
      "source": [
        "- Train = Labelled data points\n",
        "- Pool = Unlabelled data points\n",
        "- oracle = human annotator\n",
        "- uncertainity based sampling = Getting human feedback when a model is uncertain(low confidence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyzREzeNDOOX"
      },
      "source": [
        "We create a model and train it on the labelled data (we will label only a small sample but enough to have high performance). Then, we go through all the data points in the pool and identify the points that are most confusing for the classifier and add these points to the train data. We repeat this untile the models performance is high. Only used when the labelling cost is high (that is when annotating all these data points are not feasable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAA3aOplCZWA"
      },
      "source": [
        "from sklearn.svm import SVC, LinearSVC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imageio as io\n",
        "import os\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "QQFGynAtD1Yf",
        "outputId": "33fd799f-1b1d-4f20-fbfc-e5ba5f8e2e75"
      },
      "source": [
        "#origdata = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
        "origdata = pd.read_csv(\"Iris.csv\")\n",
        "origdata[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-64c65dd7d945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#origdata = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0morigdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iris.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0morigdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Iris.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wMvFqxzEIyV"
      },
      "source": [
        "This dataset contains data about 3 species/subspecies of the Iris flower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUmRCMReECnK"
      },
      "source": [
        "k1, k2 = 'petallength', 'petalwidth'\n",
        "data = origdata[[k1, k2, 'class']].copy()\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2LwzxEoEQWK"
      },
      "source": [
        "X = data[[k1, k2]]\n",
        "y = data['class']\n",
        "print('Classes:')\n",
        "print(y.unique(), '\\n\\n\\n')\n",
        "\n",
        "y[y=='Iris-setosa'] = 0\n",
        "y[y=='Iris-versicolor'] = 1\n",
        "y[y=='Iris-virginica'] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0ZrMDyrEfRC"
      },
      "source": [
        "We plot the samples of versicolor and virginica on a 2D graph with versicolor in red and virginica in cyan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih4dk9dyEw6-"
      },
      "source": [
        "plt.figure()\n",
        "setosa = y == 0\n",
        "versicolor = y == 1\n",
        "virginica = y == 2\n",
        "\n",
        "plt.scatter(X[k1][versicolor], X[k2][versicolor], c='r')\n",
        "plt.scatter(X[k1][virginica], X[k2][virginica], c='c')\n",
        "plt.xlabel(k1)\n",
        "plt.ylabel(k2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45VUEMUvE5Al"
      },
      "source": [
        "X1 = X[y != 0]\n",
        "y1 = y[y != 0]\n",
        "X1[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JTFY29lE7O0"
      },
      "source": [
        "X1 = X1.reset_index(drop=True)\n",
        "y1 = y1.reset_index(drop=True)\n",
        "y1 -= 1\n",
        "print(y1.unique())\n",
        "X1[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bKoaQP8E98N"
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "plt.scatter(X1[k1][y1==0], X1[k2][y1==0], c='r')\n",
        "plt.scatter(X1[k1][y1==1], X1[k2][y1==1], c='c')\n",
        "\n",
        "plt.xlabel(k1)\n",
        "plt.ylabel(k2)\n",
        "fig.savefig('main.jpg', dpi=100)\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecY2hEMnK60L"
      },
      "source": [
        "Linear SVM kernel\n",
        "- Linear Kernel is used when the data is Linearly separable, that is, it can be separated using a single Line\n",
        "-  The main idea is that based on the labeled data (training data) the algorithm tries to find the optimal hyperplane which can be used to classify new data points. In two dimensions the hyperplane is a simple line.\n",
        "- Based on these support vectors, the algorithm tries to find the best hyperplane that separates the classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noyew5TmFCSE"
      },
      "source": [
        "y1 = y1.astype(dtype=np.uint8)\n",
        "clf0 = LinearSVC()\n",
        "clf0.fit(X1, y1)\n",
        "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "          verbose=0)\n",
        "print(clf0.coef_)\n",
        "print(clf0.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8tdIRAKP9fb"
      },
      "source": [
        "- Formula for reference\n",
        "- a*x + b*y + c = 0\n",
        "- y = -(a*x + c)/b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz-4xgtWFGA6"
      },
      "source": [
        "xmin, xmax = X1[k1].min(), X1[k1].max()\n",
        "ymin, ymax = X1[k2].min(), X1[k2].max()\n",
        "stepx = (xmax - xmin)/99\n",
        "stepy = (ymax - ymin)/99\n",
        "a0, b0, c0 = clf0.coef_[0, 0], clf0.coef_[0, 1], clf0.intercept_\n",
        "\n",
        "\n",
        "lx0 = [xmin + stepx * i for i in range(100)]\n",
        "ly0 = [-(a0*lx0[i] + c0)/b0 for i in range(100)]\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.scatter(X1[k1][y1==0], X1[k2][y1==0], c='r')\n",
        "plt.scatter(X1[k1][y1==1], X1[k2][y1==1], c='c')\n",
        "\n",
        "plt.plot(lx0, ly0, c='m')\n",
        "\n",
        "plt.xlabel(k1)\n",
        "plt.ylabel(k2)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCYBrnDFQvv"
      },
      "source": [
        "Now, we split the dataset into two parts â€” pool(80%) and test(20%). We use a random state of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba15XtJVFLiT"
      },
      "source": [
        "X_pool, X_test, y_pool, y_test = train_test_split(X1, y1, test_size=0.2, random_state=1)\n",
        "X_pool, X_test, y_pool, y_test = X_pool.reset_index(drop=True), X_test.reset_index(drop=True), y_pool.reset_index(drop=True), y_test.reset_index(drop=True)\n",
        "# random state 1 5 iterations\n",
        "# random state 2 20 iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqMTKXGNYP0o"
      },
      "source": [
        "for a two-class linear SVM, the decision function outputs positive values for one of the classes (one side of the decision boundary) and negative values for the other class (other side of the decision boundary) and zero on the decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Y5FVMeFS5l"
      },
      "source": [
        "clf0.decision_function(X_pool.iloc[6:8])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POi8-CQ8Faup"
      },
      "source": [
        "Thus, find_most_ambiguous, gives the unlabelled point that is the closest to the decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhcr96BWYa7s"
      },
      "source": [
        "For an SVM classifier, if a data point is closer to the decision boundary and less ambiguous if the data point is farther from the decision boundary no matter which side of the decision boundary the point is on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kw_d9OhFVGc"
      },
      "source": [
        "def find_most_ambiguous(clf, unknown_indexes):\n",
        "    \n",
        "    ind = np.argmin(np.abs( \n",
        "        list(clf0.decision_function(X_pool.iloc[unknown_indexes]) )\n",
        "        ))\n",
        "    return unknown_indexes[ind]\n",
        "\n",
        "# unknown_indexes- indexes from the dataset that are the unlabelled/unknown pool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu-fdbYrY7qc"
      },
      "source": [
        "We also have the ideal decision boundary calculated earlier. This line is also plotted (in magenta).\n",
        "Finally, we plot the new_index point, that is, the most ambiguous point (yellow star)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2xPknHGFdMA"
      },
      "source": [
        "def plot_svm(clf, train_indexes, unknown_indexes, new_index = False, title = False, name = False):\n",
        "    X_train = X_pool.iloc[train_indexes]\n",
        "    y_train = y_pool.iloc[train_indexes]\n",
        "\n",
        "    X_unk = X_pool.iloc[unknown_indexes]\n",
        "\n",
        "    if new_index:\n",
        "        X_new = X_pool.iloc[new_index]\n",
        "\n",
        "    a, b, c = clf.coef_[0, 0], clf.coef_[0, 1], clf.intercept_\n",
        "    # Straight Line Formula\n",
        "    # a*x + b*y + c = 0\n",
        "    # y = -(a*x + c)/b\n",
        "\n",
        "    lx = [xmin + stepx * i for i in range(100)]\n",
        "    ly = [-(a*lx[i] + c)/b for i in range(100)]\n",
        "\n",
        "    fig = plt.figure(figsize=(9,6))\n",
        "\n",
        "    # plt.scatter(x[k1][setosa], x[k2][setosa], c='r')\n",
        "    plt.scatter(X_unk[k1], X_unk[k2], c='k', marker = '.')\n",
        "    plt.scatter(X_train[k1][y_train==0], X_train[k2][y_train==0], c='r', marker = 'o')\n",
        "    plt.scatter(X_train[k1][y_train==1], X_train[k2][y_train==1], c='c', marker = 'o')\n",
        "    \n",
        "\n",
        "    plt.plot(lx, ly, c='m')\n",
        "    plt.plot(lx0, ly0, '--', c='g')\n",
        "\n",
        "    if new_index:\n",
        "        plt.scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125)\n",
        "        plt.scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125)\n",
        "        plt.scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125)\n",
        "        plt.scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125)\n",
        "        plt.scatter(X_new[k1], X_new[k2], c='y', marker=\"*\", s=125)\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    \n",
        "    plt.xlabel(k1)\n",
        "    plt.ylabel(k2)\n",
        "\n",
        "    if name:\n",
        "        fig.set_size_inches((9,6))\n",
        "        plt.savefig(name, dpi=100)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kipOVGdheDh"
      },
      "source": [
        "train_indexes = list(range(10))\n",
        "unknown_indexes = list(range(10, 80))\n",
        "X_train = X_pool.iloc[train_indexes]\n",
        "y_train = y_pool.iloc[train_indexes]\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# folder = \"rs1it5/\"\n",
        "folder = \"rs2it20/\"\n",
        "# folder = \"rs1it20/\"\n",
        "\n",
        "try:\n",
        "    os.mkdir(folder)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# filenames = [\"ActiveLearningTitleSlide2.jpg\"] * 2\n",
        "filenames = []\n",
        "title = \"Beginning\"\n",
        "# name = folder + (\"rs1it5_0a.jpg\")\n",
        "name = folder + (\"rs2it20_0a.jpg\")\n",
        "plot_svm(clf, train_indexes, unknown_indexes, False, title, name)\n",
        "\n",
        "filenames.append(name)\n",
        "\n",
        "n = find_most_ambiguous(clf, unknown_indexes)\n",
        "unknown_indexes.remove(n)\n",
        "\n",
        "title = \"Iteration 0\"\n",
        "name = folder + (\"rs1it5_0b.jpg\")\n",
        "# name = folder + (\"rs2it20_0b.jpg\")\n",
        "filenames.append(name)\n",
        "plot_svm(clf, train_indexes, unknown_indexes, n, title, name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzDHIJVKZkHX"
      },
      "source": [
        "Next, we run the active learning algorithm for 5 iterations. In each of them, we add the most ambiguous point to the training data and train an SVM, find the most unambiguous point at this stage and then create a plot all this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4eYvaJVhkkW"
      },
      "source": [
        "num = 5\n",
        "# num = 20\n",
        "t = []\n",
        "for i in range(num):\n",
        "    \n",
        "    train_indexes.append(n)\n",
        "    X_train = X_pool.iloc[train_indexes]\n",
        "    y_train = y_pool.iloc[train_indexes]\n",
        "    clf = LinearSVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    title, name = \"Iteration \"+str(i+1), folder + (\"rs1it5_%d.jpg\" % (i+1))\n",
        "    # title, name = \"Iteration \"+str(i+1), folder + (\"rs2it20_%d.jpg\" % (i+1))\n",
        "\n",
        "    n = find_most_ambiguous(clf, unknown_indexes)\n",
        "    unknown_indexes.remove(n)\n",
        "    plot_svm(clf, train_indexes, unknown_indexes, n, title, name)\n",
        "    filenames.append(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SUi0MtdZ8xH"
      },
      "source": [
        "This is how active learning can be used to create robust models with labelling fewer data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2TcNxuPiApd"
      },
      "source": [
        "filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0sqpjVlhn0Z"
      },
      "source": [
        "images = []\n",
        "for filename in filenames[2:]:\n",
        "    images.append(io.imread(filename))\n",
        "io.mimsave('rs1it5.gif', images, duration = 1)\n",
        "# io.mimsave('rs2it20.gif', images, duration = 1)\n",
        "# io.mimsave('rs1it20.gif', images, duration = 1)\n",
        "try:\n",
        "    os.mkdir('rs1it5')\n",
        "#    os.mkdir('rt2it20')\n",
        "except:\n",
        "    pass\n",
        "os.listdir('rs1it5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyc-Hn0ohsQd"
      },
      "source": [
        "# with open('rs1it5.gif','rb') as f:\n",
        "#     display(Image(data=f.read(), format='gif'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HA2Na6yid3F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}